<html>
<head>

<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
<link href='http://fonts.googleapis.com/css?family=Lato:300,400,900' rel='stylesheet' type='text/css'>
<link href="style.css" rel="stylesheet">
<title>Efficient bandwidth extension of musical signals
using a differentiable harmonic plus noise model</title>
</head>
<body>

    <div id="header" class="container-fluid">
            <h1>Diffusion-based Spectral Super-Resolution of <br>Third Octave Acoustic Sensor Data: Is Privacy at Risk ?</h1>
   </div>


<div class="authors">
	Modan Tailleur, Chaymae Benaatia, Mathieu Lagrange, Pierre Aumond, Vincent Tourre
</div>
<div class="authors">
	Contact: modan.tailleur@ls2n.fr
</div>
<div class="container">
    <h2>Abstract</h2>
    <p>

        Third octave spectral recording of Acoustic sensor data is an effective way of measuring the environment. While there is strong evidence that slow (1s frame, 1 Hz rate) and fast (125ms frame, 8Hz rate) versions lead by-design to unintelligible speech if reconstructed, the advent of high quality reconstruction methods based on diffusion may pose a threat, as those approaches can embed a significant amount of <i>a priori</i> knowledge when learned over extensive speech datasets. 

        This paper aims to assess this risk at three levels of attacks with a growing level  of <i>a priori</i> knowledge considered at the learning of the diffusion model, a) none, b) multi-speaker data excluding the target speaker and c) target speaker. Without any prior regarding the speech profile of the speaker (levels a and b), our results suggest a rather low risk as the word-error-rate both for humans and automatic recognition remains higher than 89%.


    </p>
    <p>This is the companion page for the paper: Diffusion-based spectral super-resolution of third octave acoustic sensor data: is privacy at risk ?</p>
    <p>Please, cite as:</p>
	<p>Tailleur, M., Benaatia, C., Lagrange, M., Aumond, P., & Tourre, V. (2025). Diffusion-based spectral super-resolution of third octave acoustic sensor data: is privacy at risk?. In EUSIPCO.</p>
    <p>Experience code is available in <a href="https://github.com/modantailleur/paperThirdOctavePrivacy">this github repository</a>.</p>
</div>

<div class="container" id="table_techniques">
  <h2>Voice reconstruction from Fast Third-Octave</h2>
  <p>Bellow, the voice extracts generated from the different methods on LJ002-0068.wav audio from the LJSpeech test set.</p>
  <table class="table table-responsive">
      <thead>
        <tr class="text-center">
          <td rowspan="2"></td>
          <td rowspan="2"></td>
          <td colspan="2" style="border-left: 1px solid rgb(185, 185, 185);">AL0</td>
          <td style="border-left: 1px solid rgb(185, 185, 185);">AL1</td>
          <td colspan="1" style="border-left: 1px solid rgb(185, 185, 185);">AL2</td>
          <td rowspan="2" style="border-left: 1px solid rgb(185, 185, 185);">Reference</td>
      </tr>
      <tr class="text-center">
          <td style="border-left: 1px solid rgb(185, 185, 185);">PINV</td>
          <td >Diffspec</td>
          <td style="border-left: 1px solid rgb(185, 185, 185);">Diffspec</td>
          <!-- <td style="border-left: 1px solid rgb(185, 185, 185);">Diffwave</td> -->
          <td style="border-left: 1px solid rgb(185, 185, 185);">Diffspec</td>
      </tr>
      </thead>
      <tbody>
        <tr>
          <td class="desc">LJ002-0068.wav</td>
          <td class="desc">LJSpeech</td>
          <td class="text-center" style="border-left: 1px solid rgb(185, 185, 185);">
              <audio controls class="sample_audio">
                  <source src="audio/for_companion_page/evalset=ljspeech+method=pinv+step=vocode+tho_type=fast___gomin___LJ003-0180.wav" type="audio/wav">
              </audio>
          </td>
          <td class="text-center">
              <audio controls class="sample_audio">
                  <source src="audio/for_companion_page/dataset=tau+diff_steps=1000+epoch=40+evalset=ljspeech+learning_rate=-4+method=diffusion+schedule=DDPM+seed=71+step=vocode+tho_type=fast___gomin___LJ003-0180.wav" type="audio/wav">
              </audio>
          </td>
          <td class="text-center" style="border-left: 1px solid rgb(185, 185, 185);">
              <audio controls class="sample_audio">
                  <source src="audio/for_companion_page/dataset=librispeech+diff_steps=1000+epoch=40+evalset=ljspeech+learning_rate=-4+method=diffusion+schedule=DDPM+seed=71+step=vocode+tho_type=fast___gomin___LJ003-0180.wav" type="audio/wav">
              </audio>
          </td>
          <!-- <td class="text-center" style="border-left: 1px solid rgb(185, 185, 185);">
              <audio controls class="sample_audio">
                  <source src="audio/for_companion_page/dataset=ljspeech+diff_steps=1000+epoch=70+evalset=ljspeech+learning_rate=-4+method=diffwave+schedule=DDPM+seed=71+step=vocode+tho_type=fast___LJ003-0180.wav" type="audio/wav">
              </audio>
          </td> -->
          <td class="text-center" style="border-left: 1px solid rgb(185, 185, 185);">
              <audio controls class="sample_audio">
                  <source src="audio/for_companion_page/dataset=ljspeech+diff_steps=1000+epoch=70+evalset=ljspeech+learning_rate=-4+method=diffusion+schedule=DDPM+seed=71+step=vocode+tho_type=fast___gomin___LJ003-0180.wav" type="audio/wav">
              </audio>
          </td>
          <td class="text-center" style="border-left: 1px solid rgb(185, 185, 185);">
              <audio controls class="sample_audio">
                  <source src="audio/for_companion_page/LJ003-0180.wav" type="audio/wav">
              </audio>
          </td>
      </tr>
      <tr>
          <td class="desc">8463-294828-0003.flac</td>
          <td class="desc">Librispeech</td>
          <td class="text-center" style="border-left: 1px solid rgb(185, 185, 185);">
              <audio controls class="sample_audio">
                  <source src="audio/for_companion_page/evalset=librispeech+method=pinv+step=vocode+tho_type=fast___gomin___8463-294828-0003.wav" type="audio/wav">
              </audio>
          </td>
          <td class="text-center">
              <audio controls class="sample_audio">
                  <source src="audio/for_companion_page/dataset=tau+diff_steps=1000+epoch=40+evalset=librispeech+learning_rate=-4+method=diffusion+schedule=DDPM+seed=71+step=vocode+tho_type=fast___gomin___8463-294828-0003.wav" type="audio/wav">
              </audio>
          </td>
          <td class="text-center" style="border-left: 1px solid rgb(185, 185, 185);">
              <audio controls class="sample_audio">
                  <source src="audio/for_companion_page/dataset=librispeech+diff_steps=1000+epoch=40+evalset=librispeech+learning_rate=-4+method=diffusion+schedule=DDPM+seed=71+step=vocode+tho_type=fast___gomin___8463-294828-0003.wav" type="audio/wav">
              </audio>
          </td>
          <!-- <td class="text-center" style="border-left: 1px solid rgb(185, 185, 185);">-</td> Replaced audio with a dash -->
          <td class="text-center" style="border-left: 1px solid rgb(185, 185, 185);">-</td> 
          <td class="text-center" style="border-left: 1px solid rgb(185, 185, 185);">
              <audio controls class="sample_audio">
                  <source src="audio/for_companion_page/8463-294828-0003.wav" type="audio/wav">
              </audio>
          </td>
      </tr>
      </tbody>
    </table>
    <p>Here is the Whisper "large-v3" transcription using the PINV algorithm for the LJ002-0068.wav audio: <br>
        <i>"These words were always supposed to go on the floor. Sometimes they all went on for a boon or a comedy or a family or a friend."</i>
    </p>
    <p>And for the 8463-294828-0003.flac audio:<br>
        <i>"I'll be back in Hawaii to see my country again. I'm a friend. I'm not a scorer by the temple gardens. I really love relations."</i>
    </p>
    <p>Although most of the words are incorrect, it is notable 
    that some are still accurately transcribed, despite the PINV audios being perceptually unintelligible.
    Whisper "large-v3" model was thus removed from the discussions, as this behavior may result from overfitting 
    due to the use of LJSpeech and LibriSpeech audio in training the Whisper models.</p>
</div>

<div class="container" id="table_effects">
  <h2>Influence of diffusion steps</h2>
  <p>Below are the voice extracts generated from training on the LJSpeech dataset either with 200 diffusion steps or 1000 diffusion steps, using the LJ002-0068.wav audio from the LJSpeech test set. We observe that with 200 diffusion steps, the audio quality remains similar; however, fewer words are intelligible, and the speech becomes slightly more disjointed. The training with 1000 diffusion steps is thus retained for the experiment.</p>
  <table class="table table-responsive">
    <thead>
      <tr class="text-center">
        <td rowspan="2"></td>
        <td rowspan="2"></td>
        <td colspan="2" style="border-left: 1px solid rgb(185, 185, 185);">Diffusion steps</td>
    </tr>
    <tr class="text-center">
        <td style="border-left: 1px solid rgb(185, 185, 185);">200</td>
        <td>1000</td>
    </tr>
    </thead>
    <tbody>
      <tr>
        <td class="desc">LJ002-0068.wav</td>
        <td class="desc">LJSpeech</td>
        <td class="text-center" style="border-left: 1px solid rgb(185, 185, 185);">
            <audio controls class="sample_audio">
                <source src="audio/for_companion_page/dataset=ljspeech+diff_steps=200+epoch=70+evalset=ljspeech+learning_rate=-4+method=diffusion+schedule=DDPM+seed=71+step=vocode+tho_type=fast___gomin___LJ003-0180.wav" type="audio/wav">
            </audio>
        </td>
        <td class="text-center">
            <audio controls class="sample_audio">
                <source src="audio/for_companion_page/dataset=ljspeech+diff_steps=1000+epoch=70+evalset=ljspeech+learning_rate=-4+method=diffusion+schedule=DDPM+seed=71+step=vocode+tho_type=fast___gomin___LJ003-0180.wav" type="audio/wav">
            </audio>
        </td>
    </tr>
    </tbody>
  </table>

  
</div>

<div class="container" id="table_effects">
    <h2>Influence of the vocoder</h2>
    <p>Below are the voice extracts generated from training on the LJSpeech dataset, using either Griffin-Lim with 30 iterations as a vocoder or GomiGAN. While the intelligibility of the words is similar in both cases, the audio quality produced by GomiGAN is slightly better, and GomiGAN is 15 times faster when using a GPU:</p>
    <table class="table table-responsive">
      <thead>
        <tr class="text-center">
          <td rowspan="2"></td>
          <td rowspan="2"></td>
          <td colspan="2" style="border-left: 1px solid rgb(185, 185, 185);">Vocoder</td>
      </tr>
      <tr class="text-center">
          <td style="border-left: 1px solid rgb(185, 185, 185);">Griffin-Lim (30 iter)</td>
          <td>GomiGAN</td>
      </tr>
      </thead>
      <tbody>
        <tr>
          <td class="desc">LJ002-0068.wav</td>
          <td class="desc">LJSpeech</td>
          <td class="text-center" style="border-left: 1px solid rgb(185, 185, 185);">
              <audio controls class="sample_audio">
                  <source src="audio/for_companion_page/dataset=ljspeech+diff_steps=1000+epoch=70+evalset=ljspeech+learning_rate=-4+method=diffusion+schedule=DDPM+seed=71+step=vocode+tho_type=fast___grifflim___LJ003-0180.wav" type="audio/wav">
              </audio>
          </td>
          <td class="text-center">
              <audio controls class="sample_audio">
                  <source src="audio/for_companion_page/dataset=ljspeech+diff_steps=1000+epoch=70+evalset=ljspeech+learning_rate=-4+method=diffusion+schedule=DDPM+seed=71+step=vocode+tho_type=fast___gomin___LJ003-0180.wav" type="audio/wav">
              </audio>
          </td>
      </tr>
      </tbody>
    </table>
  
    
  </div>

  <div class="container" id="librispeech_results">
    <h2>Additional Results</h2>
    <p>In the previous section and in our paper, our models were evaluated on a subset of 100 samples from the LJSpeech dataset.
        We also evaluated these models on 100 samples from LibriSpeech, a multi-speaker dataset. The speakers in the evaluation set are not present in the LibriSpeech training set. The table below presents the Word Error Rate (WER, in %)
        for various attack levels, which combine different methods and training sets. For more details about the models, please refer to our paper. 
        Results are found to be very similar to those presented in Table II of our paper, with the same peculiar tendencies of low WER for Whisper. </p>
    <table class="table table-responsive">
        <thead>
            <tr>
                <th>Attack Level</th>
                <th>Training Set</th>
                <th>Method</th>
                <th>FairseqS2T</th>
                <th>W2V2</th>
                <th>CRDNN</th>
                <th>Whisper</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>-</td>
                <td>-</td>
                <td>Original (mel)</td>
                <td>14.84 <small>(±3.86)</small></td>
                <td>10.83 <small>(±1.91)</small></td>
                <td>11.59 <small>(±2.05)</small></td>
                <td>10.50 <small>(±1.86)</small></td>
            </tr>
            <tr>
                <td rowspan="2">AL0</td>
                <td>-</td>
                <td>PINV transc.</td>
                <td>95.76 <small>(±0.74)</small></td>
                <td>93.62 <small>(±1.12)</small></td>
                <td>97.15 <small>(±0.60)</small></td>
                <td>75.74 <small>(±4.01)</small></td>
            </tr>
            <tr>
                <td>TAU</td>
                <td>Diffspec</td>
                <td>91.58 <small>(±1.63)</small></td>
                <td>88.41 <small>(±1.63)</small></td>
                <td>93.18 <small>(±1.24)</small></td>
                <td>91.03 <small>(±2.29)</small></td>
            </tr>
            <tr>
                <td>AL1</td>
                <td>Librispeech</td>
                <td>Diffspec</td>
                <td>84.33 <small>(±2.31)</small></td>
                <td>82.32 <small>(±2.09)</small></td>
                <td>86.05 <small>(±2.33)</small></td>
                <td>82.00 <small>(±2.62)</small></td>
            </tr>
        </tbody>
    </table>
</div>


<script>
function setupCallback(elem, elems) {
  elem.addEventListener("play", function () {
    for (var other of elems) {
      if (other !== elem) {
        other.pause();
        // other.currentTime = 0.;
      }
    }
  });
}

document.addEventListener('DOMContentLoaded', function () {
  var elems = document.body.getElementsByTagName("audio");
  for (var elem of elems) {    setupCallback(elem, elems);
  }
});
</script>
</body>
</html>
